{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation on Breast cancer data from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data\n",
    "def load_data():\n",
    "    #reading csv file\n",
    "    b=pd.read_csv('L:\\Starting Neural Network\\second neural network\\data.csv')\n",
    "    c=np.array(b)\n",
    "    c[:,2:-1]= c[:,2:-1] / c[:,2:-1].max(axis=0)\n",
    "    np.random.shuffle(c)\n",
    "    train_data=c[0:400,2:-1]\n",
    "    train_detect=c[0:400,1]\n",
    "    test_data=c[400:-1,2:-1]\n",
    "    test_detect=c[400:-1,1]\n",
    "    test_label=[]\n",
    "    train_label=[]\n",
    "    for i in train_detect:\n",
    "        if i=='M':\n",
    "            train_label.append(1)\n",
    "        else:\n",
    "            train_label.append(0)\n",
    "    for i in test_detect:\n",
    "        if i=='M':\n",
    "            test_label.append(1)\n",
    "        else:\n",
    "            test_label.append(0)\n",
    "    train_label=np.array(train_label).reshape(len(train_label),1)\n",
    "    return train_data,train_label,test_data,test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self,train_data):\n",
    "        '''Seed the random number generator'''\n",
    "        np.random.seed(1)\n",
    "        l1_size=np.shape(train_data)[1]\n",
    "        hidden_l1 = 20\n",
    "        hidden_l2 = 10\n",
    "        n_class = 1\n",
    "        stddev1 = np.sqrt(2/(l1_size+hidden_l1))\n",
    "        stddev2 = np.sqrt(2/(hidden_l1+hidden_l2))\n",
    "        stddev3 = np.sqrt(2/(hidden_l2+n_class))\n",
    "        '''Initialize weghts and biases'''\n",
    "        self.synaptic_weights1 = np.random.normal(0,stddev1,[l1_size,hidden_l1])\n",
    "        self.synaptic_weights2 = np.random.normal(0,stddev2,[hidden_l1,hidden_l2])\n",
    "        self.synaptic_weights3 = np.random.normal(0,stddev3,[hidden_l2,n_class])\n",
    "        self.bias1 = 1\n",
    "        self.bias2 = 1\n",
    "        self.bias3 = 1\n",
    "        '''Gradient variables'''\n",
    "        self.grad1=0\n",
    "        self.grad2=0\n",
    "        self.grad3=0\n",
    "        self.bias_grad1=0\n",
    "        self.bias_grad2=0\n",
    "        self.bias_grad3=0\n",
    "\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Takes in weighted sum of the inputs and normalizes\n",
    "        them through between 0 and 1 through a sigmoid function\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"\n",
    "        The derivative of the sigmoid function used to\n",
    "        calculate necessary weight adjustments\n",
    "        \"\"\"\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def train(self, training_inputs, training_outputs, training_iterations, mode=\"normal\"):\n",
    "        \"\"\"\n",
    "        We train the model through trial and error, adjusting the\n",
    "        synaptic weights each time to get a better result\n",
    "        \"\"\"\n",
    "        learning_rate=0.01\n",
    "        for iteration in range(training_iterations):\n",
    "            # Pass training set through the neural network\n",
    "            inputs = training_inputs.astype(float).reshape(1,np.shape(train_data)[1])\n",
    "            a1 = self.sigmoid(np.dot(inputs, self.synaptic_weights1)+self.bias1)\n",
    "            a2 = self.sigmoid(np.dot(a1, self.synaptic_weights2)+self.bias2)\n",
    "            a3 = self.sigmoid(np.dot(a2, self.synaptic_weights3)+self.bias3)\n",
    "            # Multiply error by input and gradient of the sigmoid function\n",
    "            # Less confident weights are adjusted more through the nature of the function\n",
    "            if mode==\"normal\":\n",
    "                '''This division is for non-convex function's gradient calculation'''\n",
    "                # Calculate the error rate\n",
    "                error = (training_outputs - a3)\n",
    "                J=np.sum(error,axis=0)\n",
    "                '''weight gradient'''\n",
    "                self.grad3 = np.matmul(a2.T, error * self.sigmoid_derivative(a3))\n",
    "                self.grad2 = np.matmul(np.transpose(a1),np.matmul(error * self.sigmoid_derivative(a3),np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2))\n",
    "                upd=np.matmul(error * self.sigmoid_derivative(a3),np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2)\n",
    "                self.grad1 = np.matmul(np.transpose(inputs),(self.sigmoid_derivative(a1)*np.matmul(upd,np.transpose(self.synaptic_weights2))))\n",
    "                '''bias gradient'''\n",
    "                bias_grad3_tot = error * self.sigmoid_derivative(a3)\n",
    "                bias_grad2_tot = np.matmul(error * self.sigmoid_derivative(a3),np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2)\n",
    "                upd1=np.matmul(error * self.sigmoid_derivative(a3),np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2)\n",
    "                bias_grad1_tot = (self.sigmoid_derivative(a1)*np.matmul(upd,np.transpose(self.synaptic_weights2)))\n",
    "                self.bias_grad3 = np.mean(bias_grad3_tot,axis=0)\n",
    "                self.bias_grad2 = np.mean(bias_grad2_tot)\n",
    "                self.bias_grad1 = np.mean(bias_grad1_tot)\n",
    "                \n",
    "            else:\n",
    "                '''This division is for convex function's gradient calculation'''\n",
    "                cost = (training_outputs*np.log(a3)) + ((1-training_outputs)*np.log(1-a3))\n",
    "                J=np.sum(cost,axis=0)\n",
    "                error=a3-training_outputs\n",
    "                self.grad3 = np.matmul(a2.T, error)\n",
    "                self.grad2 = np.matmul(np.transpose(a1),np.matmul(error,np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2))\n",
    "                upd=np.matmul(error,np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2)\n",
    "                self.grad1 = np.matmul(np.transpose(inputs),(self.sigmoid_derivative(a1)*np.dot(upd,np.transpose(self.synaptic_weights2))))\n",
    "                '''bias gradient'''\n",
    "                bias_grad3_tot = error\n",
    "                bias_grad2_tot = np.matmul(error ,np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2)\n",
    "                upd1=np.matmul(error ,np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2)\n",
    "                bias_grad1_tot = (self.sigmoid_derivative(a1)*np.matmul(upd,np.transpose(self.synaptic_weights2)))\n",
    "                self.bias_grad3 = np.mean(bias_grad3_tot)\n",
    "                self.bias_grad2 = np.mean(bias_grad2_tot)\n",
    "                self.bias_grad1 = np.mean(bias_grad1_tot)\n",
    "            \n",
    "            # Adjust synaptic weights\n",
    "            self.synaptic_weights1 -= self.grad1*learning_rate\n",
    "            self.synaptic_weights2 -= self.grad2*learning_rate\n",
    "            self.synaptic_weights3 -= self.grad3*learning_rate\n",
    "            # Adjust bias\n",
    "            self.bias1 -= self.bias_grad1*learning_rate\n",
    "            self.bias2 -= self.bias_grad2*learning_rate\n",
    "            self.bias3 -= self.bias_grad3*learning_rate\n",
    "            \n",
    "        return J\n",
    "\n",
    "    def think(self, inputs):\n",
    "        \"\"\"\n",
    "        Pass inputs through the neural network to get output\n",
    "        \"\"\"\n",
    "        \n",
    "        inputs = inputs.astype(float)\n",
    "        a1 = self.sigmoid(np.dot(inputs, self.synaptic_weights1)+self.bias1)\n",
    "        a2 = self.sigmoid(np.dot(a1, self.synaptic_weights2)+self.bias2)\n",
    "        a3 = self.sigmoid(np.dot(a2, self.synaptic_weights3)+self.bias3)\n",
    "        return (a3)  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in iteration 0 : 0.7020043890102992\n",
      "Loss in iteration 1 : 0.6661643605344907\n",
      "Loss in iteration 2 : 0.665132668354751\n",
      "Loss in iteration 3 : 0.6639027489718998\n",
      "Loss in iteration 4 : 0.6624742632626043\n",
      "Loss in iteration 5 : 0.6607763038135178\n",
      "Loss in iteration 6 : 0.6587136122174018\n",
      "Loss in iteration 7 : 0.656158767303598\n",
      "Loss in iteration 8 : 0.6529446746606514\n",
      "Loss in iteration 9 : 0.648861768781451\n",
      "Loss in iteration 10 : 0.6436656457279093\n",
      "Loss in iteration 11 : 0.6370949772396517\n",
      "Loss in iteration 12 : 0.6288833138710413\n",
      "Loss in iteration 13 : 0.6187431429858804\n",
      "Loss in iteration 14 : 0.6063351324419096\n",
      "Loss in iteration 15 : 0.5912749545010062\n",
      "Loss in iteration 16 : 0.5732110996416563\n",
      "Loss in iteration 17 : 0.5519515545076655\n",
      "Loss in iteration 18 : 0.5275844587621051\n",
      "Loss in iteration 19 : 0.5005420047307004\n",
      "Loss in iteration 20 : 0.47158392744145294\n",
      "Loss in iteration 21 : 0.4417050083015722\n",
      "Loss in iteration 22 : 0.4119880914652763\n",
      "Loss in iteration 23 : 0.3834376469441901\n",
      "Loss in iteration 24 : 0.3568397523373417\n",
      "Loss in iteration 25 : 0.33268753933714484\n",
      "Loss in iteration 26 : 0.3111827717913631\n",
      "Loss in iteration 27 : 0.2922932191561386\n",
      "Loss in iteration 28 : 0.27583210164914124\n",
      "Loss in iteration 29 : 0.26153204592323204\n",
      "Loss in iteration 30 : 0.24910016951536298\n",
      "Loss in iteration 31 : 0.23825259880599303\n",
      "Loss in iteration 32 : 0.22873254069086335\n",
      "Loss in iteration 33 : 0.22031733122706182\n",
      "Loss in iteration 34 : 0.21281902956600618\n",
      "Loss in iteration 35 : 0.2060817242857315\n",
      "Loss in iteration 36 : 0.19997749318877617\n",
      "Loss in iteration 37 : 0.19440209223361435\n",
      "Loss in iteration 38 : 0.18927090719246947\n",
      "Loss in iteration 39 : 0.1845153896959102\n",
      "Loss in iteration 40 : 0.18008003310337023\n",
      "Loss in iteration 41 : 0.1759198629165697\n",
      "Loss in iteration 42 : 0.17199838291652494\n",
      "Loss in iteration 43 : 0.16828590910955132\n",
      "Loss in iteration 44 : 0.16475822609025215\n",
      "Loss in iteration 45 : 0.16139550768748215\n",
      "Loss in iteration 46 : 0.15818145227776256\n",
      "Loss in iteration 47 : 0.15510259133610393\n",
      "Loss in iteration 48 : 0.15214773704294363\n",
      "Loss in iteration 49 : 0.1493075409563796\n",
      "Loss in iteration 50 : 0.14657414097189433\n",
      "Loss in iteration 51 : 0.14394087816626533\n",
      "Loss in iteration 52 : 0.1414020687903296\n",
      "Loss in iteration 53 : 0.1389528197409761\n",
      "Loss in iteration 54 : 0.1365888783837761\n",
      "Loss in iteration 55 : 0.13430650967410454\n",
      "Loss in iteration 56 : 0.13210239518833375\n",
      "Loss in iteration 57 : 0.12997354997764574\n",
      "Loss in iteration 58 : 0.12791725414606803\n",
      "Loss in iteration 59 : 0.1259309967830125\n",
      "Loss in iteration 60 : 0.12401243039966031\n",
      "Loss in iteration 61 : 0.12215933437586791\n",
      "Loss in iteration 62 : 0.12036958616301305\n",
      "Loss in iteration 63 : 0.11864113914546819\n",
      "Loss in iteration 64 : 0.11697200616950082\n",
      "Loss in iteration 65 : 0.11536024782670998\n",
      "Loss in iteration 66 : 0.11380396464629669\n",
      "Loss in iteration 67 : 0.11230129241723205\n",
      "Loss in iteration 68 : 0.11085039993331965\n",
      "Loss in iteration 69 : 0.10944948853293435\n",
      "Loss in iteration 70 : 0.10809679288968005\n",
      "Loss in iteration 71 : 0.10679058259753653\n",
      "Loss in iteration 72 : 0.10552916418066832\n",
      "Loss in iteration 73 : 0.10431088324043347\n",
      "Loss in iteration 74 : 0.10313412652727656\n",
      "Loss in iteration 75 : 0.10199732379097519\n",
      "Loss in iteration 76 : 0.10089894931796087\n",
      "Loss in iteration 77 : 0.09983752310883781\n",
      "Loss in iteration 78 : 0.09881161168321155\n",
      "Loss in iteration 79 : 0.09781982852348017\n",
      "Loss in iteration 80 : 0.09686083418564845\n",
      "Loss in iteration 81 : 0.09593333611493168\n",
      "Loss in iteration 82 : 0.09503608820841264\n",
      "Loss in iteration 83 : 0.09416789016763738\n",
      "Loss in iteration 84 : 0.09332758668197798\n",
      "Loss in iteration 85 : 0.09251406647986556\n",
      "Loss in iteration 86 : 0.09172626128035744\n",
      "Loss in iteration 87 : 0.09096314467256539\n",
      "Loss in iteration 88 : 0.09022373094563708\n",
      "Loss in iteration 89 : 0.08950707388752584\n",
      "Loss in iteration 90 : 0.08881226556686307\n",
      "Loss in iteration 91 : 0.08813843510891846\n",
      "Loss in iteration 92 : 0.08748474747391527\n",
      "Loss in iteration 93 : 0.08685040224381009\n",
      "Loss in iteration 94 : 0.08623463242199314\n",
      "Loss in iteration 95 : 0.08563670324913612\n",
      "Loss in iteration 96 : 0.08505591103753408\n",
      "Loss in iteration 97 : 0.0844915820256851\n",
      "Loss in iteration 98 : 0.08394307125444503\n",
      "Loss in iteration 99 : 0.08340976146584406\n",
      "Loss in iteration 100 : 0.082891062025499\n",
      "Loss in iteration 101 : 0.08238640786945732\n",
      "Loss in iteration 102 : 0.08189525847626042\n",
      "Loss in iteration 103 : 0.08141709686495792\n",
      "Loss in iteration 104 : 0.0809514286197655\n",
      "Loss in iteration 105 : 0.08049778094200115\n",
      "Loss in iteration 106 : 0.08005570172986606\n",
      "Loss in iteration 107 : 0.07962475868655722\n",
      "Loss in iteration 108 : 0.07920453845709689\n",
      "Loss in iteration 109 : 0.07879464579416658\n",
      "Loss in iteration 110 : 0.07839470275310993\n",
      "Loss in iteration 111 : 0.07800434791615922\n",
      "Loss in iteration 112 : 0.07762323564581593\n",
      "Loss in iteration 113 : 0.07725103536720093\n",
      "Loss in iteration 114 : 0.07688743087908038\n",
      "Loss in iteration 115 : 0.0765321196931653\n",
      "Loss in iteration 116 : 0.07618481240119242\n",
      "Loss in iteration 117 : 0.07584523206920306\n",
      "Loss in iteration 118 : 0.07551311365836796\n",
      "Loss in iteration 119 : 0.07518820347163963\n",
      "Loss in iteration 120 : 0.07487025862546084\n",
      "Loss in iteration 121 : 0.07455904654572096\n",
      "Loss in iteration 122 : 0.07425434448711601\n",
      "Loss in iteration 123 : 0.07395593907505107\n",
      "Loss in iteration 124 : 0.07366362586920765\n",
      "Loss in iteration 125 : 0.07337720894789862\n",
      "Loss in iteration 126 : 0.07309650051233077\n",
      "Loss in iteration 127 : 0.0728213205099082\n",
      "Loss in iteration 128 : 0.0725514962757203\n",
      "Loss in iteration 129 : 0.07228686219137934\n",
      "Loss in iteration 130 : 0.0720272593603911\n",
      "Loss in iteration 131 : 0.0717725352992706\n",
      "Loss in iteration 132 : 0.07152254364363722\n",
      "Loss in iteration 133 : 0.07127714386855719\n",
      "Loss in iteration 134 : 0.07103620102242546\n",
      "Loss in iteration 135 : 0.07079958547371346\n",
      "Loss in iteration 136 : 0.0705671726699359\n",
      "Loss in iteration 137 : 0.07033884290822344\n",
      "Loss in iteration 138 : 0.07011448111691371\n",
      "Loss in iteration 139 : 0.0698939766476054\n",
      "Loss in iteration 140 : 0.06967722307714772\n",
      "Loss in iteration 141 : 0.06946411801906181\n",
      "Loss in iteration 142 : 0.06925456294392329\n",
      "Loss in iteration 143 : 0.06904846300825285\n",
      "Loss in iteration 144 : 0.06884572689149103\n",
      "Loss in iteration 145 : 0.06864626664065517\n",
      "Loss in iteration 146 : 0.06844999752229594\n",
      "Loss in iteration 147 : 0.06825683788139417\n",
      "Loss in iteration 148 : 0.06806670900685732\n",
      "Loss in iteration 149 : 0.0678795350032915\n",
      "Loss in iteration 150 : 0.067695242668745\n",
      "Loss in iteration 151 : 0.0675137613781337\n",
      "Loss in iteration 152 : 0.06733502297207504\n",
      "Loss in iteration 153 : 0.0671589616508698\n",
      "Loss in iteration 154 : 0.0669855138733878\n",
      "Loss in iteration 155 : 0.06681461826062239\n",
      "Loss in iteration 156 : 0.06664621550369407\n",
      "Loss in iteration 157 : 0.06648024827609243\n",
      "Loss in iteration 158 : 0.06631666114995739\n",
      "Loss in iteration 159 : 0.06615540051621008\n",
      "Loss in iteration 160 : 0.0659964145083534\n",
      "Loss in iteration 161 : 0.06583965292977186\n",
      "Loss in iteration 162 : 0.06568506718436594\n",
      "Loss in iteration 163 : 0.0655326102103678\n",
      "Loss in iteration 164 : 0.06538223641719001\n",
      "Loss in iteration 165 : 0.06523390162516639\n",
      "Loss in iteration 166 : 0.06508756300805184\n",
      "Loss in iteration 167 : 0.064943179038152\n",
      "Loss in iteration 168 : 0.06480070943396289\n",
      "Loss in iteration 169 : 0.06466011511020316\n",
      "Loss in iteration 170 : 0.06452135813012815\n",
      "Loss in iteration 171 : 0.06438440166002084\n",
      "Loss in iteration 172 : 0.06424920992575785\n",
      "Loss in iteration 173 : 0.06411574817135578\n",
      "Loss in iteration 174 : 0.0639839826194036\n",
      "Loss in iteration 175 : 0.06385388043329622\n",
      "Loss in iteration 176 : 0.06372540968118255\n",
      "Loss in iteration 177 : 0.0635985393015508\n",
      "Loss in iteration 178 : 0.0634732390703728\n",
      "Loss in iteration 179 : 0.06334947956973491\n",
      "Loss in iteration 180 : 0.06322723215788593\n",
      "Loss in iteration 181 : 0.06310646894063615\n",
      "Loss in iteration 182 : 0.06298716274404229\n",
      "Loss in iteration 183 : 0.0628692870883205\n",
      "Loss in iteration 184 : 0.06275281616292588\n",
      "Loss in iteration 185 : 0.06263772480274761\n",
      "Loss in iteration 186 : 0.06252398846536236\n",
      "Loss in iteration 187 : 0.062411583209299855\n",
      "Loss in iteration 188 : 0.06230048567326926\n",
      "Loss in iteration 189 : 0.06219067305630302\n",
      "Loss in iteration 190 : 0.06208212309877262\n",
      "Loss in iteration 191 : 0.0619748140642357\n",
      "Loss in iteration 192 : 0.06186872472207428\n",
      "Loss in iteration 193 : 0.06176383433088686\n",
      "Loss in iteration 194 : 0.06166012262259625\n",
      "Loss in iteration 195 : 0.061557569787241785\n",
      "Loss in iteration 196 : 0.061456156458420466\n",
      "Loss in iteration 197 : 0.06135586369934716\n",
      "Loss in iteration 198 : 0.06125667298950281\n",
      "Loss in iteration 199 : 0.06115856621184443\n",
      "Loss in iteration 200 : 0.061061525640547544\n",
      "Loss in iteration 201 : 0.06096553392925627\n",
      "Loss in iteration 202 : 0.06087057409981744\n",
      "Loss in iteration 203 : 0.06077662953147404\n",
      "Loss in iteration 204 : 0.06068368395049656\n",
      "Loss in iteration 205 : 0.06059172142023126\n",
      "Loss in iteration 206 : 0.06050072633154455\n",
      "Loss in iteration 207 : 0.06041068339364575\n",
      "Loss in iteration 208 : 0.06032157762526923\n",
      "Loss in iteration 209 : 0.0602333943461978\n",
      "Loss in iteration 210 : 0.06014611916911362\n",
      "Loss in iteration 211 : 0.060059737991758046\n",
      "Loss in iteration 212 : 0.05997423698938831\n",
      "Loss in iteration 213 : 0.05988960260751464\n",
      "Loss in iteration 214 : 0.05980582155490681\n",
      "Loss in iteration 215 : 0.0597228807968553\n",
      "Loss in iteration 216 : 0.05964076754867662\n",
      "Loss in iteration 217 : 0.0595594692694511\n",
      "Loss in iteration 218 : 0.05947897365598141\n",
      "Loss in iteration 219 : 0.05939926863696268\n",
      "Loss in iteration 220 : 0.0593203423673544\n",
      "Loss in iteration 221 : 0.05924218322294363\n",
      "Loss in iteration 222 : 0.059164779795092354\n",
      "Loss in iteration 223 : 0.05908812088565968\n",
      "Loss in iteration 224 : 0.059012195502091386\n",
      "Loss in iteration 225 : 0.05893699285266872\n",
      "Loss in iteration 226 : 0.0588625023419108\n",
      "Loss in iteration 227 : 0.058788713566121656\n",
      "Loss in iteration 228 : 0.05871561630907828\n",
      "Loss in iteration 229 : 0.05864320053785043\n",
      "Loss in iteration 230 : 0.05857145639874987\n",
      "Loss in iteration 231 : 0.058500374213400284\n",
      "Loss in iteration 232 : 0.05842994447492563\n",
      "Loss in iteration 233 : 0.058360157844249486\n",
      "Loss in iteration 234 : 0.05829100514650274\n",
      "Loss in iteration 235 : 0.05822247736753422\n",
      "Loss in iteration 236 : 0.058154565650519706\n",
      "Loss in iteration 237 : 0.058087261292666365\n",
      "Loss in iteration 238 : 0.05802055574200841\n",
      "Loss in iteration 239 : 0.05795444059428947\n",
      "Loss in iteration 240 : 0.05788890758993019\n",
      "Loss in iteration 241 : 0.05782394861107559\n",
      "Loss in iteration 242 : 0.05775955567872141\n",
      "Loss in iteration 243 : 0.05769572094991397\n",
      "Loss in iteration 244 : 0.05763243671502328\n",
      "Loss in iteration 245 : 0.05756969539508501\n",
      "Loss in iteration 246 : 0.05750748953920936\n",
      "Loss in iteration 247 : 0.05744581182205462\n",
      "Loss in iteration 248 : 0.057384655041362864\n",
      "Loss in iteration 249 : 0.05732401211555572\n",
      "Loss in iteration 250 : 0.05726387608138823\n",
      "Loss in iteration 251 : 0.05720424009165852\n",
      "Loss in iteration 252 : 0.057145097412971996\n",
      "Loss in iteration 253 : 0.057086441423557094\n",
      "Loss in iteration 254 : 0.057028265611132875\n",
      "Loss in iteration 255 : 0.05697056357082446\n",
      "Loss in iteration 256 : 0.05691332900312665\n",
      "Loss in iteration 257 : 0.05685655571191303\n",
      "Loss in iteration 258 : 0.05680023760249034\n",
      "Loss in iteration 259 : 0.05674436867969503\n",
      "Loss in iteration 260 : 0.05668894304603245\n",
      "Loss in iteration 261 : 0.056633954899856585\n",
      "Loss in iteration 262 : 0.05657939853358882\n",
      "Loss in iteration 263 : 0.0565252683319753\n",
      "Loss in iteration 264 : 0.05647155877038176\n",
      "Loss in iteration 265 : 0.0564182644131242\n",
      "Loss in iteration 266 : 0.05636537991183459\n",
      "Loss in iteration 267 : 0.05631290000386094\n",
      "Loss in iteration 268 : 0.05626081951070106\n",
      "Loss in iteration 269 : 0.05620913333646801\n",
      "Loss in iteration 270 : 0.05615783646638721\n",
      "Loss in iteration 271 : 0.05610692396532478\n",
      "Loss in iteration 272 : 0.056056390976345224\n",
      "Loss in iteration 273 : 0.056006232719298756\n",
      "Loss in iteration 274 : 0.055956444489436125\n",
      "Loss in iteration 275 : 0.05590702165605248\n",
      "Loss in iteration 276 : 0.05585795966115717\n",
      "Loss in iteration 277 : 0.05580925401817069\n",
      "Loss in iteration 278 : 0.0557609003106462\n",
      "Loss in iteration 279 : 0.05571289419101782\n",
      "Loss in iteration 280 : 0.05566523137937236\n",
      "Loss in iteration 281 : 0.055617907662244705\n",
      "Loss in iteration 282 : 0.05557091889143775\n",
      "Loss in iteration 283 : 0.05552426098286438\n",
      "Loss in iteration 284 : 0.05547792991541207\n",
      "Loss in iteration 285 : 0.05543192172982888\n",
      "Loss in iteration 286 : 0.05538623252763189\n",
      "Loss in iteration 287 : 0.055340858470035316\n",
      "Loss in iteration 288 : 0.05529579577689954\n",
      "Loss in iteration 289 : 0.055251040725700556\n",
      "Loss in iteration 290 : 0.05520658965051835\n",
      "Loss in iteration 291 : 0.05516243894104461\n",
      "Loss in iteration 292 : 0.05511858504160931\n",
      "Loss in iteration 293 : 0.05507502445022569\n",
      "Loss in iteration 294 : 0.05503175371765257\n",
      "Loss in iteration 295 : 0.05498876944647532\n",
      "Loss in iteration 296 : 0.05494606829020288\n",
      "Loss in iteration 297 : 0.05490364695238272\n",
      "Loss in iteration 298 : 0.05486150218573135\n",
      "Loss in iteration 299 : 0.054819630791281916\n",
      "Loss in iteration 300 : 0.054778029617546894\n",
      "Loss in iteration 301 : 0.05473669555969679\n",
      "Loss in iteration 302 : 0.054695625558753944\n",
      "Loss in iteration 303 : 0.054654816600800994\n",
      "Loss in iteration 304 : 0.0546142657162048\n",
      "Loss in iteration 305 : 0.054573969978853686\n",
      "Loss in iteration 306 : 0.05453392650540903\n",
      "Loss in iteration 307 : 0.05449413245457146\n",
      "Loss in iteration 308 : 0.054454585026359394\n",
      "Loss in iteration 309 : 0.054415281461401455\n",
      "Loss in iteration 310 : 0.054376219040242056\n",
      "Loss in iteration 311 : 0.05433739508265925\n",
      "Loss in iteration 312 : 0.05429880694699525\n",
      "Loss in iteration 313 : 0.054260452029499494\n",
      "Loss in iteration 314 : 0.05422232776368359\n",
      "Loss in iteration 315 : 0.05418443161968744\n",
      "Loss in iteration 316 : 0.054146761103658604\n",
      "Loss in iteration 317 : 0.054109313757140856\n",
      "Loss in iteration 318 : 0.05407208715647526\n",
      "Loss in iteration 319 : 0.05403507891221249\n",
      "Loss in iteration 320 : 0.05399828666853448\n",
      "Loss in iteration 321 : 0.053961708102687994\n",
      "Loss in iteration 322 : 0.053925340924427384\n",
      "Loss in iteration 323 : 0.053889182875468726\n",
      "Loss in iteration 324 : 0.053853231728952615\n",
      "Loss in iteration 325 : 0.05381748528891765\n",
      "Loss in iteration 326 : 0.0537819413897831\n",
      "Loss in iteration 327 : 0.05374659789584119\n",
      "Loss in iteration 328 : 0.05371145270075828\n",
      "Loss in iteration 329 : 0.05367650372708528\n",
      "Loss in iteration 330 : 0.05364174892577748\n",
      "Loss in iteration 331 : 0.053607186275721815\n",
      "Loss in iteration 332 : 0.053572813783274116\n",
      "Loss in iteration 333 : 0.053538629481803886\n",
      "Loss in iteration 334 : 0.053504631431247646\n",
      "Loss in iteration 335 : 0.05347081771767037\n",
      "Loss in iteration 336 : 0.05343718645283464\n",
      "Loss in iteration 337 : 0.05340373577377786\n",
      "Loss in iteration 338 : 0.05337046384239707\n",
      "Loss in iteration 339 : 0.05333736884504126\n",
      "Loss in iteration 340 : 0.05330444899211093\n",
      "Loss in iteration 341 : 0.05327170251766524\n",
      "Loss in iteration 342 : 0.05323912767903605\n",
      "Loss in iteration 343 : 0.05320672275644828\n",
      "Loss in iteration 344 : 0.053174486052648816\n",
      "Loss in iteration 345 : 0.053142415892540214\n",
      "Loss in iteration 346 : 0.05311051062282258\n",
      "Loss in iteration 347 : 0.053078768611640584\n",
      "Loss in iteration 348 : 0.05304718824823796\n",
      "Loss in iteration 349 : 0.05301576794261754\n",
      "Loss in iteration 350 : 0.05298450612520763\n",
      "Loss in iteration 351 : 0.052953401246534776\n",
      "Loss in iteration 352 : 0.05292245177690167\n",
      "Loss in iteration 353 : 0.05289165620607112\n",
      "Loss in iteration 354 : 0.052861013042956595\n",
      "Loss in iteration 355 : 0.05283052081531691\n",
      "Loss in iteration 356 : 0.05280017806945708\n",
      "Loss in iteration 357 : 0.05276998336993535\n",
      "Loss in iteration 358 : 0.05273993529927348\n",
      "Loss in iteration 359 : 0.052710032457674603\n",
      "Loss in iteration 360 : 0.052680273462744426\n",
      "Loss in iteration 361 : 0.052650656949218125\n",
      "Loss in iteration 362 : 0.052621181568691805\n",
      "Loss in iteration 363 : 0.05259184598935967\n",
      "Loss in iteration 364 : 0.05256264889575472\n",
      "Loss in iteration 365 : 0.05253358898849424\n",
      "Loss in iteration 366 : 0.05250466498403137\n",
      "Loss in iteration 367 : 0.05247587561440878\n",
      "Loss in iteration 368 : 0.0524472196270186\n",
      "Loss in iteration 369 : 0.05241869578436579\n",
      "Loss in iteration 370 : 0.05239030286383603\n",
      "Loss in iteration 371 : 0.05236203965746753\n",
      "Loss in iteration 372 : 0.05233390497172707\n",
      "Loss in iteration 373 : 0.052305897627289835\n",
      "Loss in iteration 374 : 0.05227801645882371\n",
      "Loss in iteration 375 : 0.05225026031477659\n",
      "Loss in iteration 376 : 0.052222628057168395\n",
      "Loss in iteration 377 : 0.052195118561385395\n",
      "Loss in iteration 378 : 0.0521677307159805\n",
      "Loss in iteration 379 : 0.05214046342247434\n",
      "Loss in iteration 380 : 0.05211331559516201\n",
      "Loss in iteration 381 : 0.052086286160922014\n",
      "Loss in iteration 382 : 0.05205937405902941\n",
      "Loss in iteration 383 : 0.05203257824097173\n",
      "Loss in iteration 384 : 0.05200589767026781\n",
      "Loss in iteration 385 : 0.051979331322290934\n",
      "Loss in iteration 386 : 0.051952878184094387\n",
      "Loss in iteration 387 : 0.05192653725423999\n",
      "Loss in iteration 388 : 0.05190030754262949\n",
      "Loss in iteration 389 : 0.05187418807033986\n",
      "Loss in iteration 390 : 0.05184817786946059\n",
      "Loss in iteration 391 : 0.05182227598293413\n",
      "Loss in iteration 392 : 0.05179648146439899\n",
      "Loss in iteration 393 : 0.05177079337803565\n",
      "Loss in iteration 394 : 0.05174521079841574\n",
      "Loss in iteration 395 : 0.05171973281035289\n",
      "Loss in iteration 396 : 0.05169435850875657\n",
      "Loss in iteration 397 : 0.05166908699848882\n",
      "Loss in iteration 398 : 0.05164391739422288\n",
      "Loss in iteration 399 : 0.051618848820305006\n",
      "Loss in iteration 400 : 0.051593880410617965\n",
      "Loss in iteration 401 : 0.051569011308446946\n",
      "Loss in iteration 402 : 0.051544240666348703\n",
      "Loss in iteration 403 : 0.05151956764602157\n",
      "Loss in iteration 404 : 0.051494991418179134\n",
      "Loss in iteration 405 : 0.05147051116242471\n",
      "Loss in iteration 406 : 0.051446126067129366\n",
      "Loss in iteration 407 : 0.05142183532931067\n",
      "Loss in iteration 408 : 0.051397638154514605\n",
      "Loss in iteration 409 : 0.051373533756698925\n",
      "Loss in iteration 410 : 0.051349521358118745\n",
      "Loss in iteration 411 : 0.051325600189214046\n",
      "Loss in iteration 412 : 0.05130176948849887\n",
      "Loss in iteration 413 : 0.051278028502452756\n",
      "Loss in iteration 414 : 0.05125437648541377\n",
      "Loss in iteration 415 : 0.05123081269947364\n",
      "Loss in iteration 416 : 0.051207336414374\n",
      "Loss in iteration 417 : 0.051183946907405437\n",
      "Loss in iteration 418 : 0.05116064346330708\n",
      "Loss in iteration 419 : 0.05113742537416899\n",
      "Loss in iteration 420 : 0.05111429193933517\n",
      "Loss in iteration 421 : 0.051091242465309196\n",
      "Loss in iteration 422 : 0.051068276265660274\n",
      "Loss in iteration 423 : 0.05104539266093203\n",
      "Loss in iteration 424 : 0.05102259097855216\n",
      "Loss in iteration 425 : 0.05099987055274371\n",
      "Loss in iteration 426 : 0.050977230724437744\n",
      "Loss in iteration 427 : 0.05095467084118768\n",
      "Loss in iteration 428 : 0.050932190257084856\n",
      "Loss in iteration 429 : 0.05090978833267573\n",
      "Loss in iteration 430 : 0.05088746443487999\n",
      "Loss in iteration 431 : 0.05086521793691036\n",
      "Loss in iteration 432 : 0.05084304821819381\n",
      "Loss in iteration 433 : 0.05082095466429387\n",
      "Loss in iteration 434 : 0.050798936666833966\n",
      "Loss in iteration 435 : 0.050776993623422514\n",
      "Loss in iteration 436 : 0.05075512493757899\n",
      "Loss in iteration 437 : 0.05073333001866126\n",
      "Loss in iteration 438 : 0.050711608281793615\n",
      "Loss in iteration 439 : 0.05068995914779689\n",
      "Loss in iteration 440 : 0.05066838204311867\n",
      "Loss in iteration 441 : 0.05064687639976566\n",
      "Loss in iteration 442 : 0.0506254416552363\n",
      "Loss in iteration 443 : 0.050604077252454846\n",
      "Loss in iteration 444 : 0.05058278263970624\n",
      "Loss in iteration 445 : 0.0505615572705724\n",
      "Loss in iteration 446 : 0.05054040060386928\n",
      "Loss in iteration 447 : 0.05051931210358486\n",
      "Loss in iteration 448 : 0.05049829123881841\n",
      "Loss in iteration 449 : 0.05047733748372006\n",
      "Loss in iteration 450 : 0.05045645031743215\n",
      "Loss in iteration 451 : 0.05043562922403113\n",
      "Loss in iteration 452 : 0.050414873692469914\n",
      "Loss in iteration 453 : 0.05039418321652182\n",
      "Loss in iteration 454 : 0.050373557294725396\n",
      "Loss in iteration 455 : 0.05035299543032906\n",
      "Loss in iteration 456 : 0.05033249713123799\n",
      "Loss in iteration 457 : 0.05031206190996079\n",
      "Loss in iteration 458 : 0.050291689283557595\n",
      "Loss in iteration 459 : 0.05027137877358839\n",
      "Loss in iteration 460 : 0.0502511299060631\n",
      "Loss in iteration 461 : 0.05023094221139055\n",
      "Loss in iteration 462 : 0.05021081522433085\n",
      "Loss in iteration 463 : 0.050190748483946106\n",
      "Loss in iteration 464 : 0.05017074153355286\n",
      "Loss in iteration 465 : 0.050150793920675917\n",
      "Loss in iteration 466 : 0.05013090519700114\n",
      "Loss in iteration 467 : 0.05011107491833098\n",
      "Loss in iteration 468 : 0.050091302644538364\n",
      "Loss in iteration 469 : 0.050071587939524015\n",
      "Loss in iteration 470 : 0.05005193037117145\n",
      "Loss in iteration 471 : 0.050032329511305235\n",
      "Loss in iteration 472 : 0.050012784935647814\n",
      "Loss in iteration 473 : 0.04999329622377869\n",
      "Loss in iteration 474 : 0.04997386295909236\n",
      "Loss in iteration 475 : 0.04995448472875886\n",
      "Loss in iteration 476 : 0.04993516112368301\n",
      "Loss in iteration 477 : 0.04991589173846558\n",
      "Loss in iteration 478 : 0.04989667617136426\n",
      "Loss in iteration 479 : 0.049877514024255866\n",
      "Loss in iteration 480 : 0.0498584049025978\n",
      "Loss in iteration 481 : 0.04983934841539178\n",
      "Loss in iteration 482 : 0.049820344175146705\n",
      "Loss in iteration 483 : 0.04980139179784254\n",
      "Loss in iteration 484 : 0.04978249090289475\n",
      "Loss in iteration 485 : 0.04976364111311939\n",
      "Loss in iteration 486 : 0.049744842054697945\n",
      "Loss in iteration 487 : 0.04972609335714355\n",
      "Loss in iteration 488 : 0.04970739465326699\n",
      "Loss in iteration 489 : 0.04968874557914377\n",
      "Loss in iteration 490 : 0.049670145774080826\n",
      "Loss in iteration 491 : 0.049651594880584515\n",
      "Loss in iteration 492 : 0.049633092544328725\n",
      "Loss in iteration 493 : 0.04961463841412275\n",
      "Loss in iteration 494 : 0.04959623214188089\n",
      "Loss in iteration 495 : 0.049577873382591564\n",
      "Loss in iteration 496 : 0.049559561794286715\n",
      "Loss in iteration 497 : 0.04954129703801211\n",
      "Loss in iteration 498 : 0.049523078777797915\n",
      "Loss in iteration 499 : 0.049504906680629616\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    '''\n",
    "    Getting data\n",
    "    '''\n",
    "    train_data,train_label,test_data,test_label = load_data()\n",
    "    iterations=500\n",
    "    #print(train_data)\n",
    "    hi=NeuralNetwork(train_data)\n",
    "    cost=[0]*iterations\n",
    "    for j in range (iterations):\n",
    "        cost_per_data=0\n",
    "        for i in range (len(train_data)):\n",
    "            cost_per_data += hi.train(train_data[i],train_label[i],1,\"log\")\n",
    "        cost[j] = abs(np.mean(cost_per_data)/len(train_data))\n",
    "        print(\"Loss in iteration\",j,\":\",cost[j])\n",
    "    plt.plot(cost)\n",
    "    plt.xlabel('Iterations', fontsize=16)\n",
    "    plt.ylabel('Cost', fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 98.25\n",
      "Test Accuracy: 96.42857142857143\n"
     ]
    }
   ],
   "source": [
    "#Finding Accuracy\n",
    "valid=(hi.think(train_data)>=0.5)*1\n",
    "test=(hi.think(test_data)>=0.5)*1\n",
    "check_train=0\n",
    "check_test=0\n",
    "j=0\n",
    "for i in range (len(valid)):\n",
    "    if valid[i]==train_label[i]:\n",
    "        check_train += 1\n",
    "    if j<len(test):\n",
    "        if test[j]==test_label[j]:\n",
    "            check_test += 1\n",
    "    j=j+1\n",
    "train_accuracy=100*(check_train/len(valid))\n",
    "test_accuracy=100*(check_test/len(test))\n",
    "\n",
    "print(\"Train Accuracy:\",train_accuracy)\n",
    "print (\"Test Accuracy:\",test_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "at 150 iteration :: Train Accuracy: 97.75       Test Accuracy: 96.42857142857143\n",
    "\n",
    "at 60 iterations :: Train Accuracy: 96.75       Test Accuracy: 96.42857142857143\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
